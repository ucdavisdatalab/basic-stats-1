# Inference

Inference means making statements about the population based on a sample. In one simple case, inference can mean using a sample to estimate the mean of the population from which it arose. Since the law of large numbers says that the sample mean tends to the population mean, we'll use the sample mean to approximate the population mean. You've already seen this in the plots where I showed the sample mean getting closer to the population mean as the sample size increased.

## Revisiting the standard normal distribution:
The distribution, density, and quantile functions are all about the population. When you're working with data, the graphs won't be as clean. When you're using methods that depend upon the data being approximately normal, you'll have to check some diagnostics. One simple but effective approach is to look at the histogram and QQ plot, and judge whether there is obvious non-normality. Here's an example from the standard normal distribution:

```{r}
# sample 50 numbers from the standard normal distribution
y = rnorm(50)

# look at the histogram - it's like the density function
hist(y)

# look at the QQ plot - it demonstrates how closely the distribution matches the quantiles of a Normal distribution.
qqnorm(y)

# plot a couple of non-normal QQ plots:
qqnorm( rexp(50) )
qqnorm( rt(50, df=2) )
```


## Looking at real data

### Normal data: using the t-distribution

In the `fosdata` package there is a dataset called `mice_pot`, which contains data from an experiment where mice were dosed with THC and then measured for motor activity as a percentage of their baseline activity. We are going to look at the group that got a medium dose of THC.

```{r}
# extract just the mice that got the medium dose of THC
mice_med = mice_pot[ mice_pot$group == 1, ]

# look at the histogram and QQ plot to assess whether the data are approximately normal
hist( mice_med$percent_of_act )
qqnorm( mice_med$percent_of_act )
```

#### Find the 80% confidence interval for the population mean

Now we are using our sample to make some determination about the population, so this is statistical inference. Our best guess of the population mean is the sample mean, `mean( mice_med$percent_of_act )`, which is 99.1%. But to get a confidence interval, we need to use the formula
$$ \bar{x} \pm t_{n-1, 0.1} * S / \sqrt{n} $$
Going piece-by-piece in this formula:

```{r}
# calculate the sample mean, sample standard deviation, and sample size:
x_bar = mean( mice_med$percent_of_act )
s = sd( mice_med$percent_of_act )
n = nrow( mice_med )

# calculate the relevant quantiles of the t-distribution.
# use the 0.1 and 0.9 quantiles because we want the 80% confidence interval
# and (1-0.8) / 2 = 0.1 and 1 - 0.1 = 0.9.
t_low = qt(0.1, df=n-1)
t_high = qt(0.9, df=n-1)

# calculate the confidence interval:
cat( "The 80% CI is (", x_bar + t_low * s / sqrt(n), ", ", x_bar + t_high * s / sqrt(n), ").")

# check your calculation with the t.test function:
t.test( mice_med$percent_of_act, conf.level=0.8 )

```

### Non-normal data: resampling

In the `fosdata` package, there is a dataset called `barnacles` that is from a study where scientists counted the number of barnacles on rocks in the intertidal zone. We'll import the data and look at its distribution:

```{r}
# calculate the number of barnacles per unit area for each sample:
barnacles$per_m = barnacles$count / barnacles$area_m

# examine the histogram and QQ plot of the barnacles per unit area:
hist( barnacles$per_m, main="barnacles per square meter" )
qqnorm( barnacles$per_m )

# does it look to you like the barnacles per unit area are distributed like a normal distribution?
```

We should conclude that the data are obviously non-normal.

#### Bootstrap-t distribution:
In this case, the observations themselves are not normally distributed so the t-distribution derived from the CLT will be only an approximation. But there is another option: resampling. This means shuffling our original data to generate new values of the t-statistic, and then working directly from this "bootstrap-t" distribution for inferences. Here is the procedure for generating your bootstrap-t distribution:

 1. Compute the sample mean $\bar{x}$.
 2. Repeat the following many times:
  - Resample from the barnacle data with replacement, calling the resample `z`.
  - Calculate the t-statistic (centered at $\bar{x}$) for this resample and call it `t_boot[[i]]`. The formula is `( mean(z) - mean(barnacles$per_m) ) / ( sd(z) / sqrt(n) )`.
 3. Get the quantiles from the bootstrap-t distribution.
 4. Calculate the confidence interval as 
 $$ (\bar{x} - t_{boot, lower} \times s / \sqrt{n}, \bar{x} + t_{boot, upper} \times s / \sqrt{n} $$

```{r}
# define the sample size and a 
B = 200
t_boot = numeric( B )

# generate the bootstrap-t distribution for this data
for (i in 1:B) {
  z = sample( barnacles$per_m, replace=TRUE )
  t_boot[[i]] = ( mean(z) - mean(barnacles$per_m) ) / ( sd(z) / sqrt(length(z)) )
}

# plot the histogram of the bootstrap-t distribution
hist(t_boot)

# extract the 0.025 and 0.975 quantiles, and annotate the histogram with them
t_lower = quantile( t_boot, 0.025 )
t_upper = quantile( t_boot, 0.975 )
abline( v = c(t_lower, t_upper), lty=3)

# calculate the confidence interval
print( round( mean(barnacles$per_m) + c(t_lower, t_upper) * sd(barnacles$per_m) / sqrt(length(barnacles$per_m)), 2 ))

# compare to the result we'd see with a t confidence interval
t.test( barnacles$per_m )
```

## Hypothesis testing
We've seen how to generate some confidence intervals for locating the population mean based on a sample. Besides confidence intervals, another main topic of statistical inference is hypothesis testing.

### Null hypothesis and p-value
Recall that in statistical inference, we are using some sample to make conclusions about a population. The sample can't tell us what the population is, since many different populations could generate the same samples. But if we have a sample and a particular idea for a population, then we can see whether the sample is unlikely, given the proposed population.

When testing a hypothesis in statistics, you propose some specific hypothesis, called the null hypothesis. Then, you must evaluate how unusual the data are, given the assumption that the null hypothesis is true. As an example, let's test whether the mice that got a medium dose of THC have population mean activity that is equal to 100. Call this specific null mean $\mu_0$. Then the t-statistic for the test is

$$t = \frac{\bar{X} - \mu_0}{S / \sqrt{n} } = \frac{99.05 - 100}{S / \sqrt{12} } = -0.125$$

When working out the confidence interval, we already saw that these data are Normally distributed and the t-statistic for these data has 11 degrees of freedom. I will therefore draw the density of a t-distribution with 11 degrees of freedom, annotated with the observed t-statistic:

```{r one-pop-t}
# plot the t density with 11 df
xx = seq(-4, 4, length.out=1000)
plot( xx, dt(xx, df=11), type='l', bty='n', ylab="density", xlab='t')

# annotate with the observed t-statistic
t = (mean(mice_med$percent_of_act) - 100) /
  (sd(mice_med$percent_of_act) / sqrt(n))
abline(v=t, lty=3, col='red', lwd=2)
```


The observed t-statistic doesn't appear to be unusual under the assumed null, because it is from the fat part of the probability density. We can quantify that idea by shading the area under the density curve that is more rare than the observed t-statistic. The area of that shading is the probability that something equally or more unlikely than the observed data would occur if the null hypothesis is true. This gets called the p-value, and when it is small, we conclude that the null hypothesis should be rejected. 

```{r one-pop-t-pval}
# plot the t density with 11 df
xx = seq(-4, 4, length.out=1000)
plot( xx, dt(xx, df=11), type='l', bty='n', ylab="density", xlab='t')

# annotate with the observed t-statistic
abline(v=t, lty=3, col='red', lwd=2)

# shade the tails
polygon(x=c(xx[xx<=t], t, min(xx)), y=c(dt(xx[xx<=t], df=11), 0, 0), col=grey(0.8))
polygon(x=c(xx[xx>-t], t, max(xx)), y=c(0, dt(xx[xx>-t], df=11), 0), col=grey(0.8))
```

Here, clearly, the shaded area is not small, so don't reject the null hypothesis. This can be done more simply with the t.test function:

```{r t.test-fun}
# test whether the population mean movement is equal to 100%]
t.test( mice_med$percent_of_act, mu=100 )
```

### Two-population test
The test I just described is a one-population test because it seeks to compare a single population against a specified standard. On the other hand, you may wish to assess the null hypothesis that the movement of mice in the high-THC group is equal to the movement of mice in the medium-THC group. This is called a two-population test, since there are two populations to compare against each other.

```{r two-pop-t.test}
#extract the samples to be compared
a = mice_pot$percent_of_act[ mice_pot$group == 1]
b = mice_pot$percent_of_act[ mice_pot$group == 3]

# check for equal variances - these are close enough
var(a)
var(b)

# check whether the high-THC mice movement is Normal
qqnorm(b)

# two pop test
t.test(a, b, var.equal=TRUE)
```

